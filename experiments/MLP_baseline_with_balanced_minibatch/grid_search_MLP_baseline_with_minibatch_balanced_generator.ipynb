{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second deep learning experiment\n",
    "\n",
    "* MLP with only a hidden layer with 300 units\n",
    "* Optimizator: SGD with learning rate at 0.01\n",
    "* Custom batch generator: balanced minibatch generator with the following parameters:\n",
    "    * positive_sample_perc: positives to sample\n",
    "    * np_ratio: negative-positive ratio in minibatch\n",
    "    * negative_perc: whether to randomly undersample negative data\n",
    "* Hyperparameters:\n",
    "    * positive_sample_perc\n",
    "    * np_ratio\n",
    "    * negative_perc\n",
    "* Sigmoid output activation function\n",
    "* Hidden layer activation: sigmoid\n",
    "* Loss function: hingeloss\n",
    "* Weights initializer: glorot uniform\n",
    "* Epochs: 10\n",
    "The training and test set are not altered\n",
    "### Libraries\n",
    "* keras:2.2.0\n",
    "* scikit-learn:0.19.1\n",
    "* pandas:0.23.0\n",
    "* numpy:1.14.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "my_seed = 2024\n",
    "\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "rn.seed(my_seed)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "log_device_placement = False\n",
    "import sys\n",
    "if \"log_device_tf\" in sys.argv: \n",
    "    log_device_placement = True\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, device_count = {\"GPU\" : 0},\n",
    "                              log_device_placement=log_device_placement)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(my_seed)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import average_precision_score\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T\n",
    "import seaborn as sns\n",
    "from bioinformatics_helpers.utils import interpolated_precision_recall_curve as pr_curve\n",
    "from bioinformatics_helpers.utils import hingesig_tf\n",
    "from bioinformatics_helpers.utils import get_mendelian_dataset\n",
    "from bioinformatics_helpers.balanced_generator import BalancedGenerator\n",
    "from bioinformatics_helpers.utils import CustomKerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la griglia dei parametri da ricercare e il numero di feature per ogni esempio e settiamo il seed numpy per i numeri casuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'positive_sample_perc': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n",
    "        'np_ratio': [0.5, 1, 1.5, 3, 5],\n",
    "        'negative_perc': [0.25, 0.50, 1]\n",
    "    }\n",
    "\n",
    "feature_per_example=26\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras per funzionare con scikit mette a disposizione la classe `KerasClassifier` che fa da wrapper. Ha bisogno di una funzione che crea e compila il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    initializer = keras.initializers.glorot_uniform(seed=my_seed)\n",
    "    model.add(Dense(\n",
    "            300, \n",
    "            input_dim=feature_per_example, \n",
    "            kernel_initializer=initializer,\n",
    "            activation=\"sigmoid\")\n",
    "             )\n",
    "    model.add(Dense(\n",
    "            1,\n",
    "            kernel_initializer=initializer,\n",
    "            activation='sigmoid'\n",
    "    ))\n",
    "    optimizer = SGD(lr=0.01, decay=0, momentum=0, nesterov=False)\n",
    "    model.compile(loss=hingesig_tf, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_mendelian_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_X = np.concatenate([train_X, test_X])\n",
    "cv_y = np.concatenate([train_y, test_y])\n",
    "train_idx = np.arange(0, len(train_X))\n",
    "test_idx = np.arange(len(train_X), len(cv_X))\n",
    "assert np.all(np.equal(cv_X[train_idx][-1], train_X[-1]))\n",
    "assert np.all(np.equal(cv_X[test_idx][-1], test_X[-1]))\n",
    "assert np.all(np.equal(cv_X[test_idx][0], test_X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la funzione di scoring, calcolando l'AUC per la precision recall curve specifichiamo `reorder=False` perchè non è una curva \"ascendente\", come nel caso della curva ROC ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prc_score(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true=y_true, probas_pred=y_pred)\n",
    "    return auc(x=recall, y=precision)\n",
    "\n",
    "scoring = {\n",
    "    'AU_PRC': make_scorer(prc_score, needs_threshold=True),\n",
    "    'AU_ROC': make_scorer(roc_auc_score, needs_threshold=True),\n",
    "    'AVG_PREC': make_scorer(average_precision_score, needs_threshold=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = BalancedGenerator\n",
    "model = CustomKerasClassifier(build_fn = create_model, generator=gen, verbose=1, shuffle=False)\n",
    "    \n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=params,\n",
    "                           scoring=scoring,\n",
    "                           refit=False,\n",
    "                           cv=[(train_idx, test_idx),(train_idx, test_idx),(train_idx, test_idx),(train_idx, test_idx)], \n",
    "                           return_train_score=True,\n",
    "                           n_jobs=1)\n",
    "grid_search.fit(cv_X, cv_y)\n",
    "#saving cv_results_\n",
    "cv_results = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "cv_results.to_csv(\"cv_results_mb_gen.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

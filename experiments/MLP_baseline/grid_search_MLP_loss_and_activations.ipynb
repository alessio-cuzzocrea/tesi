{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primo esperimento deep learning\n",
    "\n",
    "* Architettura MLP con un solo strato nascosto da 300 unità\n",
    "* Ottimizzazione tramite SGD con learning rate a 0.01 no decay e minibatch da 500 sample\n",
    "* GridSearch per cercare le migliori funzion di attivazione\\perdita:\n",
    "    * attivazioni: softmax, softplus, tanh, relu, sigmoid\n",
    "    * perdita: mean squared error, binary cross entropy, hinge loss\n",
    "* L'attivazione dell'unità di uscita la fisso con una sigmoidale\n",
    "* Inizializzatore dei pesi: glorot uniforme\n",
    "* Numero di epoche: 10\n",
    "Il training set e il test set non vengono alterati.\n",
    "### Librerie utilizzate\n",
    "* keras:2.2.0\n",
    "* scikit-learn:0.19.1\n",
    "* pandas:0.23.0\n",
    "* numpy:1.14.5\n",
    "* theano:1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#Per prima cosa importiamo tutto ciò di cui abbiamo bisogno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.optimizers import SGD\n",
    "from numpy.random import seed\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import average_precision_score\n",
    "import theano.tensor as T\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la griglia dei parametri da ricercare e il numero di feature per ogni esempio e settiamo il seed numpy per i numeri casuali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hingesig(y_true, y_pred):\n",
    "    transform_y_true = T.switch(T.eq(y_true,0), -1, y_true)\n",
    "    clipped_y_pred = T.clip(y_pred, 1e-20, 0.9999999)\n",
    "    logit = (T.log2(clipped_y_pred) - T.log2(T.sub(1., clipped_y_pred)))\n",
    "    return T.mean(T.maximum(1. - transform_y_true * logit, 0.), axis=-1)\n",
    "\n",
    "params = {\n",
    "        'loss': ['binary_crossentropy', 'mean_absolute_error', hingesig],\n",
    "        'activation': ['softmax', 'softplus', 'tanh', 'relu', 'sigmoid']\n",
    "    }\n",
    "\n",
    "feature_per_example=26\n",
    "batch_size = 500\n",
    "my_seed = 2024 #imposto un valore di seed da dare a tutti i generatori\n",
    "seed(my_seed) #seed numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras per funzionare con scikit mette a disposizione la classe `KerasClassifier` che fa da wrapper. Ha bisogno di una funzione che crea e compila il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f62ba0dbcf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(loss='binary_crossentropy', activation='tanh'):\n",
    "    model = Sequential()\n",
    "    initializer = keras.initializers.glorot_uniform(seed=my_seed)\n",
    "    model.add(Dense(\n",
    "            300, \n",
    "            input_dim=feature_per_example, \n",
    "            kernel_initializer=initializer,\n",
    "            activation=activation)\n",
    "             )\n",
    "    model.add(Dense(\n",
    "            1,\n",
    "            kernel_initializer=initializer,\n",
    "            activation='sigmoid'\n",
    "    ))\n",
    "    optimizer = SGD(lr=0.01, decay=0, momentum=0, nesterov=False)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo i dati tramite pandas e creiamo il vettore di etichette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_folder = \"/home/alessio/dati/\"\n",
    "train_set_filename = base_data_folder + \"Mendelian.train.tsv\"\n",
    "test_set_filename = base_data_folder + \"Mendelian.test.tsv\"\n",
    "\n",
    "train_X = pd.read_csv(train_set_filename, sep='\\t').values\n",
    "test_X = pd.read_csv(test_set_filename, sep='\\t').values\n",
    "#creiamo le label, nel train set i primi 356 esempi sono positivi, nel test i primi 40 sono positivi\n",
    "n_positives = 356\n",
    "n_negatives = train_X.shape[0] - n_positives\n",
    "train_y = np.concatenate((\n",
    "    np.ones(n_positives, dtype=np.int32),\n",
    "    np.zeros(n_negatives, dtype=np.int32)\n",
    "))\n",
    "n_positives = 40\n",
    "n_negatives = test_X.shape[0] - n_positives\n",
    "test_y = np.concatenate((\n",
    "    np.ones(n_positives, dtype=np.int32),\n",
    "    np.zeros(n_negatives, dtype=np.int32)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo la funzione di scoring, calcolando l'AUC per la precision recall curve specifichiamo `reorder=False` perchè non è una curva \"ascendente\", come nel caso della curva ROC ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prc_score(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true=y_true, probas_pred=y_pred)\n",
    "    return auc(x=recall, y=precision)\n",
    "\n",
    "scoring = {\n",
    "    'AU_PRC': make_scorer(prc_score, needs_threshold=True),\n",
    "    'AU_ROC': make_scorer(roc_auc_score, needs_threshold=True),\n",
    "    'AVG_PREC': make_scorer(average_precision_score, needs_threshold=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = create_model, epochs = 10, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=my_seed)\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=params, \n",
    "                           scoring=scoring,\n",
    "                           refit='AVG_PREC',\n",
    "                           cv=kfold, \n",
    "                           return_train_score=True,\n",
    "                           n_jobs=6)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print(grid_search.best_params_)\n",
    "#saving best model\n",
    "joblib.dump(grid_search.best_estimator_,'best_estimator.pkl')\n",
    "#saving cv_results_\n",
    "cv_results = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "cv_results.to_csv(\"cv_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

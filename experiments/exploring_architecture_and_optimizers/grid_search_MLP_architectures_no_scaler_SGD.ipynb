{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un MLP più semplice per la predizione\n",
    "\n",
    "l'MLP di questo esperimento avrà fissati i seguenti parametri:\n",
    "* Ottimizzatore: SGD con i parametri di default\n",
    "* funzione di perdita: hingeloss\n",
    "* funzione di attivazione: sigmoid\n",
    "* architettura della rete: { [(2), (5), (10), (20), (40), (80),(100),(100, 80), (100, 40), (100, 10), (40, 20), (40,10), (20,10), (20,5), (10, 5), (10,2), (100, 80, 40), (100,40,20),(80,40,20), (80, 20,10), (40,20,10),(20,10,5), (10,5,2), (100,80,50,20), (100,50,25,10), (80, 60, 20,10), (50, 30, 20, 10),(30,15,7,3) ]}\n",
    "                           \n",
    "* numero di epoche: 150\n",
    "* dimensione batch: 5000, così abbiamo una maggiore probabilità di avere un positivo nel minibatch\n",
    "\n",
    "Il training set verrà normalizzato in modo da avere media 0 e varianza unitaria. Se l'esperimento va bene è consigliabile provare ad effettuare una feature reduction.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 2024\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(my_seed)\n",
    "import random as rn\n",
    "rn.seed(my_seed)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(my_seed)\n",
    "\n",
    "log_device_placement = False\n",
    "import sys\n",
    "if \"log_device_tf\" in sys.argv: \n",
    "    log_device_placement = True\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=0, inter_op_parallelism_threads=0, device_count = {\"GPU\" : 0},\n",
    "                              log_device_placement=log_device_placement)\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.optimizers import Adam, SGD, Adadelta\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from bioinformatics_helpers.utils import get_mendelian_dataset\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, LeakyReLU\n",
    "from bioinformatics_helpers.utils import hingesig_tf\n",
    "from bioinformatics_helpers.utils import ExhaustiveSearch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {\"model__architecture\" : [(2,), (5,), (10,), (20,), (40,), (80,),(100,),(100, 80), (100, 40), (100, 10), (40, 20), (40,10), (20,10), (20,5), (10, 5), (10,2), (100, 80, 40), (100,40,20),(80,40,20), (80, 20,10), (40,20,10),(20,10,5), (10,5,2), (100,80,50,20), (100,50,25,10), (80, 60, 20,10), (50, 30, 20, 10),(30,15,7,3) ]}\n",
    "feature_per_example = 26\n",
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(architecture=(10,)):\n",
    "    model = Sequential()\n",
    "    weights_initializer = keras.initializers.glorot_normal(seed=my_seed)\n",
    "    bias_init = keras.initializers.RandomNormal(mean=0.1, stddev=0.05, seed=my_seed)\n",
    "    input_dim = feature_per_example\n",
    "    for units in architecture:\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units,\n",
    "                input_dim = input_dim,\n",
    "                kernel_initializer = weights_initializer,\n",
    "                bias_initializer = bias_init,\n",
    "                activation=\"relu\"\n",
    "            )\n",
    "        )\n",
    "    model.add(\n",
    "        Dense(\n",
    "            1,\n",
    "            kernel_initializer=weights_initializer,\n",
    "            bias_initializer=keras.initializers.zeros(),\n",
    "            activation='sigmoid'\n",
    "    ))\n",
    "    optimizer = SGD()\n",
    "    model.compile(loss=hingesig_tf, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_mendelian_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prc_score(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true=y_true, probas_pred=y_pred)\n",
    "    return auc(x=recall, y=precision)\n",
    "\n",
    "scoring = {\n",
    "    'AVG_PREC': make_scorer(average_precision_score, needs_threshold=True),\n",
    "    'AU_PRC' : make_scorer(prc_score, needs_threshold=True),\n",
    "    'AU_ROC' : make_scorer(roc_auc_score, needs_threshold=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=1, shuffle=True, batch_size=batch_size, epochs=150)\n",
    "pipe = Pipeline([(\"model\",model)])\n",
    "cv = StratifiedKFold(n_splits=5, random_state=my_seed, shuffle=True)\n",
    "grid_search = GridSearchCV(estimator=pipe,param_grid=params,\n",
    "                           scoring=scoring,\n",
    "                           return_train_score=True,\n",
    "                           cv=cv,\n",
    "                           refit=False\n",
    "                           )\n",
    "grid_search.fit(train_X, train_y)\n",
    "#saving cv_results_\n",
    "cv_results = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "cv_results.to_csv(\"cv_results_no_scaler_SGD.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
